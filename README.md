Statistical Analysis and Machine Learning
===

This repo will serve as a holding area for any incredibly useful links, takeaways, and insights I accumulate as I continue to read and analyze the use of predictive analysis in the finance market


Organization
---

Organize main ideas into different folders with their own unique README files. As this resource continues to grow we can migrate it off of Github.

* [Data Science](https://github.com/igolden/stats-and-machine-learning/tree/master/data-science)
* [Machine Learning](https://github.com/igolden/stats-and-machine-learning/tree/master/machine-learning)
* [Quantative Analysis](https://github.com/igolden/stats-and-machine-learning/tree/master/quantatative-analysis)


Technology
---

* [Apache Spark](http://spark.apache.org/) - Apache Spark is the most powerful live streaming data processing engine available (my opinion after some basic research). The only downside, for me, is that it doesn't have a javascript of ruby API. It has java, scala, python, and R APIs.
* [Apache Hive](https://hive.apache.org/) - The data storage platform to use when your DB will have over 100M+ rows
* [Scrapy - Data Scraping with Python](http://doc.scrapy.org/en/latest/intro/overview.html) - Scrapy is a Python package that is used to scrape pages. It's the equivalent to 
* [Scrape.it - Harvest data without code](https://scrape.it/) - Could be a good option for getting a CSV up and running quickly
* [Frontera - Scraper Framework](http://blog.scrapinghub.com/2015/04/22/frontera-the-brain-behind-the-crawls/) - Used alongside Scrapy, you can use frontera to schedule and run repeat jobs while changing your scrape strategy along the way.

Data Sources
---

* [List of all sectors in stock market](http://biz.yahoo.com/p/) - could be a test dataset to have in CSV on my computer.



Useful links
---

* [Big Data Impacting Wall Street](http://radar.oreilly.com/2015/05/data-science-makes-an-impact-on-wall-street.html) - Great article about using machine learning and sentiment analysis of up to 1.2 Million records a day
* [Scraping with Node.js](https://scotch.io/tutorials/scraping-the-web-with-node-js) - Great article about using machine learning and sentiment analysis of up to 1.2 Million records a day
* [Scraping with Node.js and YQL](http://code.tutsplus.com/tutorials/web-scraping-with-node-js--net-25560)
* [Create a Twitter Sentiment Analysis](http://datascienceplus.com/how-to-create-a-twitter-sentiment-analysis-using-r-and-shiny/)
* [Aggregating and Analyzing Data](http://stats.stackexchange.com/questions/645/best-ways-to-aggregate-and-analyze-data)
